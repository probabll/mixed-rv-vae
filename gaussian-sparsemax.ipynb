{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking Multivariate Normal CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone --recursive https://github.com/SebastienMarmin/torch-mvnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"torch-mvnorm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as td\n",
    "from mvnorm import multivariate_normal_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as td\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.exp_family import ExponentialFamily\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from torch.distributions.utils import _standard_normal, broadcast_all\n",
    "from entmax import sparsemax\n",
    "\n",
    "\n",
    "class GaussianSparsemax(td.Distribution):\n",
    "    \n",
    "    arg_constraints = {\n",
    "        'loc': constraints.real, \n",
    "        'scale': constraints.positive\n",
    "    }    \n",
    "    support = td.constraints.simplex\n",
    "    has_rsample = True\n",
    "    \n",
    "    @classmethod\n",
    "    def all_faces(K):\n",
    "        \"\"\"Generate a list of 2**K - 1 bit vectors indicating all possible faces of a K-dimensional simplex.\"\"\"\n",
    "        return list(product([0, 1], repeat=K))[1:]\n",
    "\n",
    "    def __init__(self, loc, scale, validate_args=None):\n",
    "        self.loc, self.scale = broadcast_all(loc, scale)\n",
    "        batch_shape, event_shape = self.loc.shape[:-1], self.loc.shape[-1:]\n",
    "        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(GaussianSparsemax, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        new.loc = self.loc.expand(batch_shape + self.event_shape)\n",
    "        new.scale = self.scale.expand(batch_shape + self.event_shape)\n",
    "        super(GaussianSparsemax, new).__init__(batch_shape, self.event_shape, validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        # sample_shape + batch_shape + (K,)\n",
    "        z = td.Normal(loc=self.loc, scale=self.scale).rsample(sample_shape)\n",
    "        return sparsemax(z, dim=-1)\n",
    "    \n",
    "    def log_prob(self, y, pivot_alg='first', tiny=1e-12, huge=1e12):\n",
    "        K = y.shape[-1]\n",
    "        # [B, K]\n",
    "        loc = self.loc\n",
    "        scale = self.scale\n",
    "        var = scale ** 2\n",
    "        \n",
    "        # The face contains the set of coordinates greater than zero\n",
    "        # [B, K]\n",
    "        face = y > 0 \n",
    "\n",
    "        # Chose a pivot coordinate (a non-zero coordinate)\n",
    "        # [B]\n",
    "        if pivot_alg == 'first':\n",
    "            ind_pivot = torch.argmax((face > 0).float(), -1)\n",
    "        elif pivot_alg == 'random':\n",
    "            ind_pivot = td.Categorical(\n",
    "                probs=face.float()/(face.float().sum(-1, keepdims=True))\n",
    "            ).sample()\n",
    "        # Select a batch of pivots \n",
    "        # [B, K]\n",
    "        pivot_indicator = torch.nn.functional.one_hot(ind_pivot, K).bool()\n",
    "        # All non-zero coordinates but the pivot\n",
    "        # [B, K]\n",
    "        others = torch.logical_xor(face, pivot_indicator)\n",
    "        # The value of the pivot coordinate\n",
    "        # [B]\n",
    "        t = (y * pivot_indicator.float()).sum(-1)\n",
    "        # Pivot mean and variance\n",
    "        # [B]\n",
    "        t_mean = torch.where(pivot_indicator, loc, torch.zeros_like(loc)).sum(-1)\n",
    "        t_var = torch.where(pivot_indicator, var, torch.zeros_like(var)).sum(-1)\n",
    "\n",
    "        # Difference with respect to the pivot\n",
    "        # [B, K]\n",
    "        y_diff = torch.where(others, y - t.unsqueeze(-1), torch.zeros_like(y))\n",
    "        # [B, K]\n",
    "        mean_diff = torch.where(\n",
    "            others, \n",
    "            loc - t_mean.unsqueeze(-1),\n",
    "            torch.zeros_like(loc)\n",
    "        )\n",
    "        \n",
    "        # Joint log pdf for the non-zeros\n",
    "        # [B, K, K]    \n",
    "        diag = torch.diag_embed(torch.where(others, var, torch.ones_like(var)))\n",
    "        offset = t_var.unsqueeze(-1).unsqueeze(-1)\n",
    "        # We need a multivariate normal for the non-zero coordinates in `other`\n",
    "        # but to batch mvns we will need to use K-by-K covariances\n",
    "        # we can do so by embedding the lower-dimensional mvn in a higher dimensional mvn\n",
    "        # with cov=I.\n",
    "        # [B, K, K]\n",
    "        cov_mask = others.unsqueeze(-1) * others.unsqueeze(-2)\n",
    "        cov = torch.where(cov_mask, diag + offset, diag)\n",
    "        # This computes log prob of y[other] under  the lower dimensional mvn\n",
    "        # times log N(0|0,1) for the other dimensions\n",
    "        # [B]\n",
    "        log_prob = td.MultivariateNormal(mean_diff, cov).log_prob(y_diff)\n",
    "        # so we discount the contribution from the masked coordinates\n",
    "        # [B, K]\n",
    "        log_prob0 = td.Normal(torch.zeros_like(mean_diff), torch.ones_like(mean_diff)).log_prob(torch.zeros_like(y_diff)) \n",
    "        # [B]\n",
    "        log_prob = log_prob - torch.where(others, torch.zeros_like(log_prob0), log_prob0).sum(-1)\n",
    "\n",
    "        # Joint log prob for the zeros (needs the cdf)\n",
    "        # [B]\n",
    "        constant_term = 1. / torch.where(face, 1./var, torch.zeros_like(var)).sum(-1)\n",
    "        # Again, we aim to reason with lower-dimensional mvns via \n",
    "        # the td.MultivariateNormal interface. For that, I will mask the coordinates in face.\n",
    "        # The non-zeros get a tiny variance\n",
    "        # [B, K, K]\n",
    "        diag_corrected = torch.diag_embed(torch.where(face, torch.zeros_like(var) + tiny, var)) \n",
    "        # [B, 1, 1]\n",
    "        offset_corrected = constant_term.unsqueeze(-1).unsqueeze(-1)\n",
    "        # These are the zeros only.\n",
    "        # [B, K, K]\n",
    "        cov_corrected_mask = torch.logical_not(face).unsqueeze(-1) * torch.logical_not(face.unsqueeze(-2))    \n",
    "        cov_corrected = torch.where(cov_corrected_mask, diag_corrected + offset_corrected, diag_corrected)    \n",
    "\n",
    "        # The non-zeros get a large negative mean.\n",
    "        # [B]\n",
    "        mean_constant_term = constant_term * torch.where(face, (y - loc)/var, torch.zeros_like(y)).sum(-1)\n",
    "        # [B, K]\n",
    "        #  see that for non-zeros I move the location to something extremely negative\n",
    "        #  in combination with tiny variace this makes the density of 0 evaluate to 0\n",
    "        #  and the cdf of 0 evaluate to 1, for those coordinates\n",
    "        mean_corrected = torch.where(face, torch.zeros_like(y) - huge, loc + mean_constant_term.unsqueeze(-1))\n",
    "\n",
    "        # [B]\n",
    "        cdf = multivariate_normal_cdf(\n",
    "            torch.zeros_like(y),\n",
    "            mean_corrected, cov_corrected\n",
    "        )\n",
    "        log_cdf = cdf.log()\n",
    "        #print('log_cdf', log_cdf.device)\n",
    "        #if log_cdf.device != log_prob.device:\n",
    "        #    print(\"changing device\")\n",
    "        #    log_cdf = torch.tensor(log_cdf, device=log_prob.device)\n",
    "\n",
    "        # [B]\n",
    "        log_det = face.float().sum(-1).log()\n",
    "\n",
    "        # [B]\n",
    "        return log_prob + log_cdf + log_det\n",
    "    \n",
    "    def log_prob_IS(self, y, imp_samples=1000):\n",
    "\n",
    "        assert y.shape[:-1] == self.batch_shape, \"For now I need the same batch_shape\"\n",
    "        \n",
    "        batch_shape = y.shape[:-1]\n",
    "        K = self.loc.shape[-1]\n",
    "\n",
    "        # [B]\n",
    "        dtau = td.Normal(\n",
    "            loc=torch.zeros(batch_shape, device=self.loc.device), \n",
    "            scale=torch.ones(batch_shape, device=self.scale.device)\n",
    "        )\n",
    "        # [B, K]\n",
    "        dnu = td.LogNormal(\n",
    "            loc=torch.zeros(batch_shape + (K,), device=self.loc.device), \n",
    "            scale=torch.ones(batch_shape + (K,), device=self.scale.device) * 3\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # [B]\n",
    "        n = (y > 0).float().sum(-1)\n",
    "        \n",
    "        # [S, B, K]\n",
    "        y = y.expand((imp_samples,) + batch_shape + (K,))\n",
    "        \n",
    "        # [S, B, K]\n",
    "        antisupp = y == 0\n",
    "        # [S, B]\n",
    "        n_antisupp = antisupp.float().sum(-1)\n",
    "        # [S, B]\n",
    "        taus = dtau.sample(sample_shape=(imp_samples,))\n",
    "        # [S, B, K]\n",
    "        nus = dnu.sample(sample_shape=(imp_samples,))\n",
    "\n",
    "        \n",
    "        # [S, B, K] \n",
    "        X = y + taus.unsqueeze(-1)\n",
    "        X = X - torch.where(antisupp, nus, torch.zeros_like(nus))\n",
    "\n",
    "        # this just asserts we are sampling over the correct set\n",
    "        # print(\"Error: \", torch.sum((sparsemax(X, 1) - y) ** 2))\n",
    "\n",
    "        # probability of each of these Xs\n",
    "        # [S, B]\n",
    "        logp_X = td.Normal(loc=self.loc, scale=self.scale).log_prob(X).sum(-1)\n",
    "\n",
    "        # probability of our importance distribution (iid over tau and nu)\n",
    "        # [S, B]\n",
    "        logq_X = dtau.log_prob(taus) \n",
    "        # [S, B]\n",
    "        logq_X += torch.where(antisupp, dnu.log_prob(nus), torch.zeros_like(nus)).sum(-1)\n",
    "\n",
    "        # \\int_s p(x) dx = E_p [1_S] = E_q[1_S * p/q]\n",
    "        # 1_S is 1 by construction\n",
    "\n",
    "        ## this multiplication by n here gives the expected result\n",
    "        ##  - in the 2d case\n",
    "        ##  - in the 3d case when looking at dimension 1- and 2- faces.\n",
    "        ## WHERE DOES IT COME FROM? I discovered it just randomly.\n",
    "        ## Can we check if this gives correct values on the interior on the 3-simplex?\n",
    "        # [B]\n",
    "        log_prob = (logp_X - logq_X).logsumexp(0) - np.log(imp_samples)\n",
    "        log_prob += n.log()\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "\n",
    "@td.register_kl(GaussianSparsemax, GaussianSparsemax)\n",
    "def _kl_gaussiansparsemax_gaussiansparsemax(p, q):\n",
    "    x = p.rsample()\n",
    "    return p.log_prob(x) - q.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density(y, base_dists, ind_pivot=0):\n",
    "    \"\"\"\n",
    "    Evaluate the density of y. Can optionally pick a pivot.\n",
    "    y_nz contains only the non-zeros, corresponding to the non-masked\n",
    "    entries of the face.\n",
    "    \"\"\"\n",
    "    face = y > 0\n",
    "    y_nz = y[np.nonzero(face)[0]]\n",
    "    from scipy.stats import multivariate_normal\n",
    "\n",
    "    nz = np.nonzero(face)[0]\n",
    "    zeros = np.nonzero(1-np.array(face))[0]\n",
    "    pivot = nz[ind_pivot]\n",
    "    nz_minus_pivot = np.array(list(set(nz) - {pivot}))\n",
    "    y = np.zeros(len(face))\n",
    "    y[nz] = y_nz\n",
    "\n",
    "    # Compute density contribution by the nonzeros - requires pdf.\n",
    "    y_diff = np.array([y[s] - y[pivot] for s in nz_minus_pivot])\n",
    "    mean_diff = np.array([base_dists[s].mean() - base_dists[pivot].mean() for s in nz_minus_pivot])\n",
    "    if len(nz_minus_pivot):\n",
    "        cov = np.diag(np.array([base_dists[s].var() for s in nz_minus_pivot])) + base_dists[pivot].var()\n",
    "        val = multivariate_normal(mean=mean_diff, cov=cov).pdf(y_diff)\n",
    "    else:\n",
    "        val = 1\n",
    "    \n",
    "    # Compute density contribution by the zeros - requires cdf.\n",
    "    if len(zeros):\n",
    "        constant_term = 1 / sum([1/base_dists[s].var() for s in nz])\n",
    "        cov_corrected = np.diag(np.array([base_dists[r].var() for r in zeros])) + constant_term\n",
    "        mean_constant_term = (constant_term *\n",
    "                              sum(np.array([(y[s] - base_dists[s].mean())/base_dists[s].var() for s in nz])))\n",
    "        mean_corrected = np.array([base_dists[r].mean() + mean_constant_term for r in zeros])\n",
    "        cdf = multivariate_normal(mean=mean_corrected, cov=cov_corrected).cdf(np.zeros(len(zeros)))\n",
    "        val *= cdf    \n",
    "    val *= sum(face)  # This is the determinant of the transformation Y -> U = g(Y) for the nonzeros.\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2711, 0.2591, 0.4698, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.3222, 0.0000, 0.6778, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0322, 0.1678, 0.1935, 0.6065],\n",
       "        [0.6300, 0.0000, 0.3700, 0.0000, 0.0000],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
       "        [0.0049, 0.0000, 0.9951, 0.0000, 0.0000]], device='cuda:1')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = GaussianSparsemax(\n",
    "    loc=td.Normal(0., 1.).sample((8, 5,)).to(torch.device('cuda:1')), \n",
    "    scale=td.Gamma(1., 1.).sample((8, 5,)).to(torch.device('cuda:1'))/4\n",
    ")\n",
    "x = p.sample()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for n in range(x.shape[0]):\n",
    "    rows.append([' '.join(f\"{v:.2f}\" for v in x.cpu().numpy()[n])])\n",
    "    rows[-1].append(np.log(density(x[n].cpu().numpy(), [st.norm(loc=u.cpu().numpy(), scale=s.cpu().numpy()) for u, s in zip(p.loc[n], p.scale[n])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(p.log_prob(x).cpu().numpy()):\n",
    "    rows[i].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(p.log_prob_IS(x, imp_samples=1000000).cpu().numpy()):\n",
    "    rows[i].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x                           scipy cdf    torch cdf         IS\n",
      "------------------------  -----------  -----------  ---------\n",
      "0.00 0.27 0.26 0.47 0.00  -1.56208     -1.56208     -1.62073\n",
      "0.00 0.00 0.00 0.00 1.00  -0.0575853   -0.0575857   -9.74892\n",
      "0.32 0.00 0.68 0.00 0.00  -1.20729     -1.20728     -1.23103\n",
      "0.00 0.03 0.17 0.19 0.61   0.607136     0.607135     0.604179\n",
      "0.63 0.00 0.37 0.00 0.00  -0.41017     -0.410169    -0.588468\n",
      "1.00 0.00 0.00 0.00 0.00  -0.00712432  -0.00711828   0.611064\n",
      "0.00 0.00 1.00 0.00 0.00  -1.19413     -1.19413     -5.86074\n",
      "0.00 0.00 1.00 0.00 0.00   0.03753      0.0375299    0.167144\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate(rows, headers=['x', 'scipy cdf', 'torch cdf', 'IS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=5 K=1\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=1\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=2\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=2\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=3\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=3\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4d86c0f163af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcheck_undefined_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mcheck_grad_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mcheck_batched_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" pass gradient check:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad)\u001b[0m\n\u001b[1;32m    399\u001b[0m             'but none of the them have requires_grad=True.')\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mfunc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtupled_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_differentiable_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-4d86c0f163af>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(values, locs, scales, reduce)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mGaussianSparsemax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-7fd436cb6670>\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, y, pivot_alg, tiny, huge)\u001b[0m\n\u001b[1;32m    135\u001b[0m         cdf = multivariate_normal_cdf(\n\u001b[1;32m    136\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mmean_corrected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_corrected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    139\u001b[0m         \u001b[0mlog_cdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/multivariate_normal_cdf.py\u001b[0m in \u001b[0;36mmultivariate_normal_cdf\u001b[0;34m(value, loc, covariance_matrix, diagonality_tolerance)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mm_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mc_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatrix_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPhi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/Phi.py\u001b[0m in \u001b[0;36mPhi\u001b[0;34m(m, c)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mPhi1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPhinception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/Phi.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, m, c)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mc_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mres_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperrectangle_integration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/integration.py\u001b[0m in \u001b[0;36mhyperrectangle_integration\u001b[0;34m(mean, covariance, lower, upper, info)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_integration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/integration.py\u001b[0m in \u001b[0;36mparallel_integration\u001b[0;34m(l, u, m, c)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     p = Parallel(n_jobs=integration.n_jobs)(\n\u001b[0;32m---> 29\u001b[0;31m         delayed(integrate)(l[j,...], u[j,...], m[j,...], c[j,...]) for j in range(N)) \n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mthis_batch_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         self.parallel._backend.batch_completed(self.batch_size,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "for K in [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 10, 10]:\n",
    "    print(f\"batch_size={batch_size} K={K}\")\n",
    "    all_y = []\n",
    "    all_u = []\n",
    "    all_s = []\n",
    "    all_r1 = []\n",
    "    for n in range(batch_size):\n",
    "        u = torch.tensor(np.random.normal(0., 1., size=K), requires_grad=True, device=torch.device('cuda:0'))\n",
    "        s = torch.tensor(np.random.normal(1., 1., size=K), requires_grad=True, device=torch.device('cuda:0'))\n",
    "        s = torch.nn.functional.softplus(s)\n",
    "        all_u.append(u)\n",
    "        all_s.append(s)\n",
    "        y = GaussianSparsemax(u, s).sample()\n",
    "        all_y.append(y)\n",
    "        all_r1.append(np.log(density(y.detach().cpu().numpy(), [st.norm(loc=u.detach().cpu().numpy()[k], scale=s.detach().cpu().numpy()[k]) for k in range(K)])))    \n",
    "\n",
    "    # make a batch\n",
    "    Y = torch.stack(all_y)\n",
    "    U = torch.stack(all_u)\n",
    "    S = torch.stack(all_s)\n",
    "    all_r2 = GaussianSparsemax(U, S).log_prob(Y, pivot_alg='first')\n",
    "    \n",
    "    all_r3 = GaussianSparsemax(U, S).log_prob(Y, pivot_alg='random')\n",
    "\n",
    "    print(\" pass pivot='first':\", torch.isclose(torch.tensor(all_r1), all_r2.cpu(), atol=1e-3).all().item())\n",
    "    print(\" pass pivot='random':\", torch.isclose(torch.tensor(all_r1), all_r3.cpu(), atol=1e-3).all().item())\n",
    "    \n",
    "    def get_loss(values, locs, scales, reduce=True):\n",
    "        loss = - GaussianSparsemax(locs, scales).log_prob(values)\n",
    "        return loss.mean() if reduce else loss\n",
    "    \n",
    "    grad_check = torch.autograd.gradcheck(\n",
    "        get_loss, \n",
    "        [Y, U, S], \n",
    "        eps=1e-02, \n",
    "        atol=0.1, \n",
    "        rtol=0.001, \n",
    "        raise_exception=True, \n",
    "        check_sparse_nnz=False, \n",
    "        nondet_tol=0.1, \n",
    "        check_undefined_grad=True, \n",
    "        check_grad_dtypes=False, \n",
    "        check_batched_grad=False\n",
    "    )\n",
    "    print(\" pass gradient check:\", grad_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = GaussianSparsemax(td.Normal(0., 1.).sample((10,)), td.Gamma(1., 1.).sample((10,)))\n",
    "q = GaussianSparsemax(td.Normal(0., 1.).sample((10,)), td.Gamma(1., 1.).sample((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.2173e-05), tensor(-2.0148e-06, dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p, p), td.kl_divergence(p.expand((100,)), p.expand((100,))).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2190)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf, dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p.expand((1000,)), q.expand((1000,))).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = p.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf, dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p.log_prob(x) - q.log_prob(x)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import probabll.distributions as pd\n",
    "\n",
    "class GaussianSparsemaxPrior(td.Distribution):\n",
    "    \n",
    "    def __init__(self, pF, alpha_net, validate_args=False):\n",
    "        self.pF = pF\n",
    "        self.alpha_net = alpha_net\n",
    "        batch_shape, event_shape = pF.batch_shape, pF.event_shape        \n",
    "        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(GaussianSparsemaxPrior, _instance)\n",
    "        new.pF = self.pF.expand(batch_shape)\n",
    "        new.alpha_net = self.alpha_net\n",
    "        super(GaussianSparsemaxPrior, new).__init__(batch_shape, self.event_shape, validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new        \n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        f = self.pF.sample(sample_shape)\n",
    "        Y = pd.MaskedDirichlet(f.bool(), self.alpha_net(f)) \n",
    "        return Y.sample()\n",
    "        \n",
    "    def log_prob(self, value):        \n",
    "        f = (value > 0).float()       \n",
    "        Y = pd.MaskedDirichlet(f.bool(), self.alpha_net(f)) \n",
    "        return self.pF.log_prob(f) + Y.log_prob(value)        \n",
    "    \n",
    "@td.register_kl(GaussianSparsemax, GaussianSparsemaxPrior)\n",
    "def _kl_gaussiansparsemax_gaussiansparsemaxprior(p, q):\n",
    "    x = p.rsample()\n",
    "    return p.log_prob(x) - q.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianSparsemaxPrior(\n",
    "    #pd.MaxEntropyFaces(pd.MaxEntropyFaces.pmf_n(4, 1)),\n",
    "    pd.NonEmptyBitVector(torch.zeros(4)),\n",
    "    torch.ones_like\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = GaussianSparsemax(td.Normal(0., 1.).sample((4,)), td.Gamma(1., 1.).sample((4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3057)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(posterior, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.4524, 0.0000, 0.5476],\n",
       "        [0.0000, 0.0000, 0.8698, 0.1302],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2357, 0.7163, 0.0480, 0.0000],\n",
       "        [0.0322, 0.2654, 0.6303, 0.0722],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0304, 0.2614, 0.7082, 0.0000],\n",
       "        [0.0581, 0.0089, 0.4966, 0.4363],\n",
       "        [0.0000, 0.2969, 0.2348, 0.4683],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior.expand((10,)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6913, 2.2989, 1.2426, 1.0014, 2.2007, 0.4468, 1.4961, 0.4468, 2.2316,\n",
       "        1.3618], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(posterior.expand((10,)), prior.expand((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
