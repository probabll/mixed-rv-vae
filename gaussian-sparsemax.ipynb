{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking Multivariate Normal CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone --recursive https://github.com/SebastienMarmin/torch-mvnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"torch-mvnorm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as td\n",
    "from mvnorm import multivariate_normal_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as td\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.exp_family import ExponentialFamily\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from torch.distributions.utils import _standard_normal, broadcast_all\n",
    "from entmax import sparsemax\n",
    "\n",
    "\n",
    "class GaussianSparsemax(td.Distribution):\n",
    "    \n",
    "    arg_constraints = {\n",
    "        'loc': constraints.real, \n",
    "        'scale': constraints.positive\n",
    "    }    \n",
    "    support = td.constraints.simplex\n",
    "    has_rsample = True\n",
    "    \n",
    "    @classmethod\n",
    "    def all_faces(K):\n",
    "        \"\"\"Generate a list of 2**K - 1 bit vectors indicating all possible faces of a K-dimensional simplex.\"\"\"\n",
    "        return list(product([0, 1], repeat=K))[1:]\n",
    "\n",
    "    def __init__(self, loc, scale, validate_args=None):\n",
    "        self.loc, self.scale = broadcast_all(loc, scale)\n",
    "        batch_shape, event_shape = self.loc.shape[:-1], self.loc.shape[-1:]\n",
    "        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(GaussianSparsemax, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        new.loc = self.loc.expand(batch_shape + self.event_shape)\n",
    "        new.scale = self.scale.expand(batch_shape + self.event_shape)\n",
    "        super(GaussianSparsemax, new).__init__(batch_shape, self.event_shape, validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        # sample_shape + batch_shape + (K,)\n",
    "        z = td.Normal(loc=self.loc, scale=self.scale).rsample(sample_shape)\n",
    "        return sparsemax(z, dim=-1)\n",
    "    \n",
    "    def IS(self, loc, cov, diag, sample_size):\n",
    "        q = td.Normal(loc=loc, scale=diag)\n",
    "        p = td.MultivariateNormal(loc, cov, validate_args=False)\n",
    "        # [S, B, K]\n",
    "        x = q.sample((sample_size,))\n",
    "        x = torch.where(x > 0, -x, x)        \n",
    "        # [S, B]\n",
    "        log_prob = (p.log_prob(x) - 2.0*q.log_prob(x).sum(-1)).logsumexp(0) - np.log(sample_size)\n",
    "        return log_prob        \n",
    "    \n",
    "    def log_prob(self, y, pivot_alg='first', tiny=1e-9, huge=1e9):\n",
    "        K = y.shape[-1]\n",
    "        # [B, K]\n",
    "        loc = self.loc\n",
    "        scale = self.scale\n",
    "        var = scale ** 2\n",
    "        \n",
    "        # The face contains the set of coordinates greater than zero\n",
    "        # [B, K]\n",
    "        face = y > 0 \n",
    "\n",
    "        # Chose a pivot coordinate (a non-zero coordinate)\n",
    "        # [B]\n",
    "        if pivot_alg == 'first':\n",
    "            ind_pivot = torch.argmax((face > 0).float(), -1)\n",
    "        elif pivot_alg == 'random':\n",
    "            ind_pivot = td.Categorical(\n",
    "                probs=face.float()/(face.float().sum(-1, keepdims=True))\n",
    "            ).sample()\n",
    "        # Select a batch of pivots \n",
    "        # [B, K]\n",
    "        pivot_indicator = torch.nn.functional.one_hot(ind_pivot, K).bool()\n",
    "        # All non-zero coordinates but the pivot\n",
    "        # [B, K]\n",
    "        others = torch.logical_xor(face, pivot_indicator)\n",
    "        # The value of the pivot coordinate\n",
    "        # [B]\n",
    "        t = (y * pivot_indicator.float()).sum(-1)\n",
    "        # Pivot mean and variance\n",
    "        # [B]\n",
    "        t_mean = torch.where(pivot_indicator, loc, torch.zeros_like(loc)).sum(-1)\n",
    "        t_var = torch.where(pivot_indicator, var, torch.zeros_like(var)).sum(-1)\n",
    "\n",
    "        # Difference with respect to the pivot\n",
    "        # [B, K]\n",
    "        y_diff = torch.where(others, y - t.unsqueeze(-1), torch.zeros_like(y))\n",
    "        # [B, K]\n",
    "        mean_diff = torch.where(\n",
    "            others, \n",
    "            loc - t_mean.unsqueeze(-1),\n",
    "            torch.zeros_like(loc)\n",
    "        )\n",
    "        \n",
    "        # Joint log pdf for the non-zeros\n",
    "        # [B, K, K]    \n",
    "        diag = torch.diag_embed(torch.where(others, var, torch.ones_like(var)))\n",
    "        offset = t_var.unsqueeze(-1).unsqueeze(-1)\n",
    "        # We need a multivariate normal for the non-zero coordinates in `other`\n",
    "        # but to batch mvns we will need to use K-by-K covariances\n",
    "        # we can do so by embedding the lower-dimensional mvn in a higher dimensional mvn\n",
    "        # with cov=I.\n",
    "        # [B, K, K]\n",
    "        cov_mask = others.unsqueeze(-1) * others.unsqueeze(-2)\n",
    "        cov = torch.where(cov_mask, diag + offset, diag)\n",
    "        # This computes log prob of y[other] under  the lower dimensional mvn\n",
    "        # times log N(0|0,1) for the other dimensions\n",
    "        # [B]\n",
    "        log_prob = td.MultivariateNormal(mean_diff, cov).log_prob(y_diff)\n",
    "        # so we discount the contribution from the masked coordinates\n",
    "        # [B, K]\n",
    "        log_prob0 = td.Normal(torch.zeros_like(mean_diff), torch.ones_like(mean_diff)).log_prob(torch.zeros_like(y_diff)) \n",
    "        # [B]\n",
    "        log_prob = log_prob - torch.where(others, torch.zeros_like(log_prob0), log_prob0).sum(-1)\n",
    "\n",
    "        # Joint log prob for the zeros (needs the cdf)\n",
    "        # [B]\n",
    "        constant_term = 1. / torch.where(face, 1./var, torch.zeros_like(var)).sum(-1)\n",
    "        # Again, we aim to reason with lower-dimensional mvns via \n",
    "        # the td.MultivariateNormal interface. For that, I will mask the coordinates in face.\n",
    "        # The non-zeros get a tiny variance\n",
    "        # [B, K, K]\n",
    "        diag_corrected = torch.diag_embed(torch.where(face, torch.zeros_like(var) + tiny, var)) \n",
    "        # [B, 1, 1]\n",
    "        offset_corrected = constant_term.unsqueeze(-1).unsqueeze(-1)\n",
    "        # These are the zeros only.\n",
    "        # [B, K, K]\n",
    "        cov_corrected_mask = torch.logical_not(face).unsqueeze(-1) * torch.logical_not(face.unsqueeze(-2))    \n",
    "        cov_corrected = torch.where(cov_corrected_mask, diag_corrected + offset_corrected, diag_corrected)    \n",
    "\n",
    "        # The non-zeros get a large negative mean.\n",
    "        # [B]\n",
    "        mean_constant_term = constant_term * torch.where(face, (y - loc)/var, torch.zeros_like(y)).sum(-1)\n",
    "        # [B, K]\n",
    "        #  see that for non-zeros I move the location to something extremely negative\n",
    "        #  in combination with tiny variace this makes the density of 0 evaluate to 0\n",
    "        #  and the cdf of 0 evaluate to 1, for those coordinates\n",
    "        mean_corrected = torch.where(face, torch.zeros_like(y) - huge, loc + mean_constant_term.unsqueeze(-1))\n",
    "\n",
    "        # [B]\n",
    "        cdf = multivariate_normal_cdf(\n",
    "            torch.zeros_like(y),\n",
    "            mean_corrected, cov_corrected\n",
    "        )        \n",
    "        log_cdf = cdf.log()\n",
    "        #try:\n",
    "        print(cdf)\n",
    "        print(self.IS(mean_corrected, cov_corrected, torch.where(face, torch.zeros_like(var) + tiny, var), 100000))\n",
    "        #except:\n",
    "        #    print(\"error\")\n",
    "        # log F(0; u, S)\n",
    "        # log \\int_{x<0} MVN(x|u,S) dx\n",
    "        # log \\int_{x<0} q(x) MVN(x|u,S)/q(x) dx\n",
    "        # log E[ MVN(x|u,S) / q(x)]\n",
    "        # \n",
    "        #print('log_cdf', log_cdf.device)\n",
    "        #if log_cdf.device != log_prob.device:\n",
    "        #    print(\"changing device\")\n",
    "        #    log_cdf = torch.tensor(log_cdf, device=log_prob.device)\n",
    "\n",
    "        # [B]\n",
    "        log_det = face.float().sum(-1).log()\n",
    "\n",
    "        # [B]\n",
    "        return log_prob + log_cdf + log_det\n",
    "    \n",
    "    def log_prob_IS(self, y, imp_samples=1000):\n",
    "\n",
    "        assert y.shape[:-1] == self.batch_shape, \"For now I need the same batch_shape\"\n",
    "        \n",
    "        batch_shape = y.shape[:-1]\n",
    "        K = self.loc.shape[-1]\n",
    "\n",
    "        # [B]\n",
    "        dtau = td.Normal(\n",
    "            loc=torch.zeros(batch_shape, device=self.loc.device), \n",
    "            scale=torch.ones(batch_shape, device=self.scale.device)\n",
    "        )\n",
    "        # [B, K]\n",
    "        dnu = td.LogNormal(\n",
    "            loc=torch.zeros(batch_shape + (K,), device=self.loc.device), \n",
    "            scale=torch.ones(batch_shape + (K,), device=self.scale.device) * 3\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # [B]\n",
    "        n = (y > 0).float().sum(-1)\n",
    "        \n",
    "        # [S, B, K]\n",
    "        y = y.expand((imp_samples,) + batch_shape + (K,))\n",
    "        \n",
    "        # [S, B, K]\n",
    "        antisupp = y == 0\n",
    "        # [S, B]\n",
    "        n_antisupp = antisupp.float().sum(-1)\n",
    "        # [S, B]\n",
    "        taus = dtau.sample(sample_shape=(imp_samples,))\n",
    "        # [S, B, K]\n",
    "        nus = dnu.sample(sample_shape=(imp_samples,))\n",
    "\n",
    "        \n",
    "        # [S, B, K] \n",
    "        X = y + taus.unsqueeze(-1)\n",
    "        X = X - torch.where(antisupp, nus, torch.zeros_like(nus))\n",
    "\n",
    "        # this just asserts we are sampling over the correct set\n",
    "        # print(\"Error: \", torch.sum((sparsemax(X, 1) - y) ** 2))\n",
    "\n",
    "        # probability of each of these Xs\n",
    "        # [S, B]\n",
    "        logp_X = td.Normal(loc=self.loc, scale=self.scale).log_prob(X).sum(-1)\n",
    "\n",
    "        # probability of our importance distribution (iid over tau and nu)\n",
    "        # [S, B]\n",
    "        logq_X = dtau.log_prob(taus) \n",
    "        # [S, B]\n",
    "        logq_X += torch.where(antisupp, dnu.log_prob(nus), torch.zeros_like(nus)).sum(-1)\n",
    "\n",
    "        # \\int_s p(x) dx = E_p [1_S] = E_q[1_S * p/q]\n",
    "        # 1_S is 1 by construction\n",
    "\n",
    "        ## this multiplication by n here gives the expected result\n",
    "        ##  - in the 2d case\n",
    "        ##  - in the 3d case when looking at dimension 1- and 2- faces.\n",
    "        ## WHERE DOES IT COME FROM? I discovered it just randomly.\n",
    "        ## Can we check if this gives correct values on the interior on the 3-simplex?\n",
    "        # [B]\n",
    "        log_prob = (logp_X - logq_X).logsumexp(0) - np.log(imp_samples)\n",
    "        log_prob += n.log()\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "\n",
    "@td.register_kl(GaussianSparsemax, GaussianSparsemax)\n",
    "def _kl_gaussiansparsemax_gaussiansparsemax(p, q):\n",
    "    x = p.rsample()\n",
    "    return p.log_prob(x) - q.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density(y, base_dists, ind_pivot=0):\n",
    "    \"\"\"\n",
    "    Evaluate the density of y. Can optionally pick a pivot.\n",
    "    y_nz contains only the non-zeros, corresponding to the non-masked\n",
    "    entries of the face.\n",
    "    \"\"\"\n",
    "    face = y > 0\n",
    "    y_nz = y[np.nonzero(face)[0]]\n",
    "    from scipy.stats import multivariate_normal\n",
    "\n",
    "    nz = np.nonzero(face)[0]\n",
    "    zeros = np.nonzero(1-np.array(face))[0]\n",
    "    pivot = nz[ind_pivot]\n",
    "    nz_minus_pivot = np.array(list(set(nz) - {pivot}))\n",
    "    y = np.zeros(len(face))\n",
    "    y[nz] = y_nz\n",
    "\n",
    "    # Compute density contribution by the nonzeros - requires pdf.\n",
    "    y_diff = np.array([y[s] - y[pivot] for s in nz_minus_pivot])\n",
    "    mean_diff = np.array([base_dists[s].mean() - base_dists[pivot].mean() for s in nz_minus_pivot])\n",
    "    if len(nz_minus_pivot):\n",
    "        cov = np.diag(np.array([base_dists[s].var() for s in nz_minus_pivot])) + base_dists[pivot].var()\n",
    "        val = multivariate_normal(mean=mean_diff, cov=cov).pdf(y_diff)\n",
    "    else:\n",
    "        val = 1\n",
    "    \n",
    "    # Compute density contribution by the zeros - requires cdf.\n",
    "    if len(zeros):\n",
    "        constant_term = 1 / sum([1/base_dists[s].var() for s in nz])\n",
    "        cov_corrected = np.diag(np.array([base_dists[r].var() for r in zeros])) + constant_term\n",
    "        mean_constant_term = (constant_term *\n",
    "                              sum(np.array([(y[s] - base_dists[s].mean())/base_dists[s].var() for s in nz])))\n",
    "        mean_corrected = np.array([base_dists[r].mean() + mean_constant_term for r in zeros])\n",
    "        cdf = multivariate_normal(mean=mean_corrected, cov=cov_corrected).cdf(np.zeros(len(zeros)))\n",
    "        val *= cdf    \n",
    "    val *= sum(face)  # This is the determinant of the transformation Y -> U = g(Y) for the nonzeros.\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7339, 0.0000, 0.0000, 0.0000, 0.2661],\n",
       "        [0.8412, 0.0000, 0.0000, 0.1588, 0.0000],\n",
       "        [0.0000, 0.0000, 0.3623, 0.6377, 0.0000],\n",
       "        [0.0000, 0.4134, 0.4350, 0.1516, 0.0000],\n",
       "        [0.7539, 0.0000, 0.2461, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.5136, 0.0418, 0.4445],\n",
       "        [0.0000, 0.0000, 0.3882, 0.6118, 0.0000],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:1')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = GaussianSparsemax(\n",
    "    loc=td.Normal(0., 1.).sample((8, 5,)).to(torch.device('cuda:1')), \n",
    "    scale=td.Gamma(1., 1.).sample((8, 5,)).to(torch.device('cuda:1'))/4\n",
    ")\n",
    "x = p.sample()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8842, 1.0000, 0.9960, 0.9208, 0.0353, 1.0000, 0.9889, 0.6450],\n",
      "       device='cuda:1', dtype=torch.float64)\n",
      "tensor([-55.3063, -57.9804, -65.7051, -89.7737,  82.6449, -90.6306, -63.9083,\n",
      "        -29.0333], device='cuda:1')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3013,  2.4685,  1.5994, -0.2397, -2.7674, -1.1220, -0.1170, -0.4385],\n",
       "       device='cuda:1', dtype=torch.float64)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for n in range(x.shape[0]):\n",
    "    rows.append([' '.join(f\"{v:.2f}\" for v in x.cpu().numpy()[n])])\n",
    "    rows[-1].append(np.log(density(x[n].cpu().numpy(), [st.norm(loc=u.cpu().numpy(), scale=s.cpu().numpy()) for u, s in zip(p.loc[n], p.scale[n])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.7109, 0.7755, 1.0000, 0.2994, 1.0000, 1.0000, 0.9406],\n",
      "       device='cuda:1', dtype=torch.float64)\n",
      "torch.Size([8, 5]) torch.Size([8, 5, 5]) torch.Size([8, 5])\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(p.log_prob(x).cpu().numpy()):\n",
    "    rows[i].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(p.log_prob_IS(x, imp_samples=1000000).cpu().numpy()):\n",
    "    rows[i].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x                           scipy cdf     torch cdf         IS\n",
      "------------------------  -----------  ------------  ---------\n",
      "0.00 1.00 0.00 0.00 0.00  -3.3606e-08  -5.10443e-07  -1.83669\n",
      "0.70 0.00 0.00 0.30 0.00  -0.317364    -0.317364     -2.01974\n",
      "0.88 0.00 0.06 0.00 0.06   1.9305       1.9305        1.95394\n",
      "0.74 0.00 0.00 0.00 0.26   1.14177      1.14177       1.36368\n",
      "0.00 0.00 1.00 0.00 0.00  -1.20585     -1.20586      -1.19314\n",
      "0.00 0.42 0.39 0.19 0.00   2.90539      2.90539       2.96719\n",
      "0.00 0.00 0.20 0.80 0.00  -1.1042      -1.1042       -2.25844\n",
      "0.00 0.00 0.71 0.29 0.00   0.355046     0.355047      0.343735\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate(rows, headers=['x', 'scipy cdf', 'torch cdf', 'IS']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=5 K=1\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=1\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=2\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=2\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=3\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=3\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4d86c0f163af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mcheck_undefined_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mcheck_grad_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mcheck_batched_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" pass gradient check:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/torch/autograd/gradcheck.py\u001b[0m in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception, check_sparse_nnz, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad)\u001b[0m\n\u001b[1;32m    399\u001b[0m             'but none of the them have requires_grad=True.')\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mfunc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtupled_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_differentiable_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-4d86c0f163af>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(values, locs, scales, reduce)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mGaussianSparsemax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-7fd436cb6670>\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, y, pivot_alg, tiny, huge)\u001b[0m\n\u001b[1;32m    135\u001b[0m         cdf = multivariate_normal_cdf(\n\u001b[1;32m    136\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mmean_corrected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_corrected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    139\u001b[0m         \u001b[0mlog_cdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/multivariate_normal_cdf.py\u001b[0m in \u001b[0;36mmultivariate_normal_cdf\u001b[0;34m(value, loc, covariance_matrix, diagonality_tolerance)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mm_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvector_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mc_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatrix_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPhi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/Phi.py\u001b[0m in \u001b[0;36mPhi\u001b[0;34m(m, c)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mPhi1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPhinception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/Phi.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, m, c)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mc_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mres_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperrectangle_integration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/integration.py\u001b[0m in \u001b[0;36mhyperrectangle_integration\u001b[0;34m(mean, covariance, lower, upper, info)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_integration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/probabll/mixed-rv-vae/deps/torch-mvnorm/mvnorm/integration.py\u001b[0m in \u001b[0;36mparallel_integration\u001b[0;34m(l, u, m, c)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     p = Parallel(n_jobs=integration.n_jobs)(\n\u001b[0;32m---> 29\u001b[0;31m         delayed(integrate)(l[j,...], u[j,...], m[j,...], c[j,...]) for j in range(N)) \n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/mixedrv/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mthis_batch_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         self.parallel._backend.batch_completed(self.batch_size,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "for K in [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 10, 10]:\n",
    "    print(f\"batch_size={batch_size} K={K}\")\n",
    "    all_y = []\n",
    "    all_u = []\n",
    "    all_s = []\n",
    "    all_r1 = []\n",
    "    for n in range(batch_size):\n",
    "        u = torch.tensor(np.random.normal(0., 1., size=K), requires_grad=True, device=torch.device('cuda:0'))\n",
    "        s = torch.tensor(np.random.normal(1., 1., size=K), requires_grad=True, device=torch.device('cuda:0'))\n",
    "        s = torch.nn.functional.softplus(s)\n",
    "        all_u.append(u)\n",
    "        all_s.append(s)\n",
    "        y = GaussianSparsemax(u, s).sample()\n",
    "        all_y.append(y)\n",
    "        all_r1.append(np.log(density(y.detach().cpu().numpy(), [st.norm(loc=u.detach().cpu().numpy()[k], scale=s.detach().cpu().numpy()[k]) for k in range(K)])))    \n",
    "\n",
    "    # make a batch\n",
    "    Y = torch.stack(all_y)\n",
    "    U = torch.stack(all_u)\n",
    "    S = torch.stack(all_s)\n",
    "    all_r2 = GaussianSparsemax(U, S).log_prob(Y, pivot_alg='first')\n",
    "    \n",
    "    all_r3 = GaussianSparsemax(U, S).log_prob(Y, pivot_alg='random')\n",
    "\n",
    "    print(\" pass pivot='first':\", torch.isclose(torch.tensor(all_r1), all_r2.cpu(), atol=1e-3).all().item())\n",
    "    print(\" pass pivot='random':\", torch.isclose(torch.tensor(all_r1), all_r3.cpu(), atol=1e-3).all().item())\n",
    "    \n",
    "    def get_loss(values, locs, scales, reduce=True):\n",
    "        loss = - GaussianSparsemax(locs, scales).log_prob(values)\n",
    "        return loss.mean() if reduce else loss\n",
    "    \n",
    "    grad_check = torch.autograd.gradcheck(\n",
    "        get_loss, \n",
    "        [Y, U, S], \n",
    "        eps=1e-02, \n",
    "        atol=0.1, \n",
    "        rtol=0.001, \n",
    "        raise_exception=True, \n",
    "        check_sparse_nnz=False, \n",
    "        nondet_tol=0.1, \n",
    "        check_undefined_grad=True, \n",
    "        check_grad_dtypes=False, \n",
    "        check_batched_grad=False\n",
    "    )\n",
    "    print(\" pass gradient check:\", grad_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = GaussianSparsemax(td.Normal(0., 1.).sample((10,)), td.Gamma(1., 1.).sample((10,)))\n",
    "q = GaussianSparsemax(td.Normal(0., 1.).sample((10,)), td.Gamma(1., 1.).sample((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.2173e-05), tensor(-2.0148e-06, dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p, p), td.kl_divergence(p.expand((100,)), p.expand((100,))).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2190)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf, dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p.expand((1000,)), q.expand((1000,))).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = p.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf, dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p.log_prob(x) - q.log_prob(x)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import probabll.distributions as pd\n",
    "\n",
    "class GaussianSparsemaxPrior(td.Distribution):\n",
    "    \n",
    "    def __init__(self, pF, alpha_net, validate_args=False):\n",
    "        self.pF = pF\n",
    "        self.alpha_net = alpha_net\n",
    "        batch_shape, event_shape = pF.batch_shape, pF.event_shape        \n",
    "        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(GaussianSparsemaxPrior, _instance)\n",
    "        new.pF = self.pF.expand(batch_shape)\n",
    "        new.alpha_net = self.alpha_net\n",
    "        super(GaussianSparsemaxPrior, new).__init__(batch_shape, self.event_shape, validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new        \n",
    "        \n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        f = self.pF.sample(sample_shape)\n",
    "        Y = pd.MaskedDirichlet(f.bool(), self.alpha_net(f)) \n",
    "        return Y.sample()\n",
    "        \n",
    "    def log_prob(self, value):        \n",
    "        f = (value > 0).float()       \n",
    "        Y = pd.MaskedDirichlet(f.bool(), self.alpha_net(f)) \n",
    "        return self.pF.log_prob(f) + Y.log_prob(value)        \n",
    "    \n",
    "@td.register_kl(GaussianSparsemax, GaussianSparsemaxPrior)\n",
    "def _kl_gaussiansparsemax_gaussiansparsemaxprior(p, q):\n",
    "    x = p.rsample()\n",
    "    return p.log_prob(x) - q.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = GaussianSparsemaxPrior(\n",
    "    #pd.MaxEntropyFaces(pd.MaxEntropyFaces.pmf_n(4, 1)),\n",
    "    pd.NonEmptyBitVector(torch.zeros(4)),\n",
    "    torch.ones_like\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = GaussianSparsemax(td.Normal(0., 1.).sample((4,)), td.Gamma(1., 1.).sample((4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3057)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(posterior, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.4524, 0.0000, 0.5476],\n",
       "        [0.0000, 0.0000, 0.8698, 0.1302],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.2357, 0.7163, 0.0480, 0.0000],\n",
       "        [0.0322, 0.2654, 0.6303, 0.0722],\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0304, 0.2614, 0.7082, 0.0000],\n",
       "        [0.0581, 0.0089, 0.4966, 0.4363],\n",
       "        [0.0000, 0.2969, 0.2348, 0.4683],\n",
       "        [0.0000, 0.0000, 0.0000, 1.0000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior.expand((10,)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6913, 2.2989, 1.2426, 1.0014, 2.2007, 0.4468, 1.4961, 0.4468, 2.2316,\n",
       "        1.3618], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(posterior.expand((10,)), prior.expand((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import probabll.distributions as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAbsTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Transform via the mapping :math:`y = |x|`.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/envs/mixedrv/lib/python3.7/site-packages/torch/distributions/transforms.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = td.transforms.AbsTransform()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(torch.ones(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "Np = td.TransformedDistribution(\n",
    "    td.Normal(torch.zeros(1), torch.ones(1)),\n",
    "    td.transforms.AbsTransform()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(Np.sample((10000,)).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Np.log_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
