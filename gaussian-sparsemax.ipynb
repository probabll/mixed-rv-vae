{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking Multivariate Normal CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone --recursive https://github.com/SebastienMarmin/torch-mvnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"torch-mvnorm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as td\n",
    "from mvnorm import multivariate_normal_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as td\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.function import once_differentiable\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.exp_family import ExponentialFamily\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from torch.distributions.utils import _standard_normal, broadcast_all\n",
    "from entmax import sparsemax\n",
    "\n",
    "\n",
    "class GaussianSparsemax(td.Distribution):\n",
    "    \n",
    "    arg_constraints = {\n",
    "        'loc': constraints.real, \n",
    "        'scale': constraints.positive\n",
    "    }    \n",
    "    support = td.constraints.simplex\n",
    "    has_rsample = True\n",
    "    \n",
    "    @classmethod\n",
    "    def all_faces(K):\n",
    "        \"\"\"Generate a list of 2**K - 1 bit vectors indicating all possible faces of a K-dimensional simplex.\"\"\"\n",
    "        return list(product([0, 1], repeat=K))[1:]\n",
    "\n",
    "    def __init__(self, loc, scale, validate_args=None):\n",
    "        self.loc, self.scale = broadcast_all(loc, scale)\n",
    "        batch_shape, event_shape = self.loc.shape[:-1], self.loc.shape[-1:]\n",
    "        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n",
    "\n",
    "    def expand(self, batch_shape, _instance=None):\n",
    "        new = self._get_checked_instance(GaussianSparsemax, _instance)\n",
    "        batch_shape = torch.Size(batch_shape)\n",
    "        new.loc = self.loc.expand(batch_shape + self.event_shape)\n",
    "        new.scale = self.scale.expand(batch_shape + self.event_shape)\n",
    "        super().__init__(batch_shape, self.event_shape, validate_args=False)\n",
    "        new._validate_args = self._validate_args\n",
    "        return new\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        # sample_shape + batch_shape + (K,)\n",
    "        z = td.Normal(loc=self.loc, scale=self.scale).rsample(sample_shape)\n",
    "        return sparsemax(z, dim=-1)\n",
    "    \n",
    "    def log_prob(self, y, pivot_alg='first', tiny=1e-12, huge=1e12):\n",
    "        K = y.shape[-1]\n",
    "        # [B, K]\n",
    "        loc = self.loc\n",
    "        scale = self.scale\n",
    "        var = scale ** 2\n",
    "        \n",
    "        # The face contains the set of coordinates greater than zero\n",
    "        # [B, K]\n",
    "        face = y > 0 \n",
    "\n",
    "        # Chose a pivot coordinate (a non-zero coordinate)\n",
    "        # [B]\n",
    "        if pivot_alg == 'first':\n",
    "            ind_pivot = torch.argmax((face > 0).float(), -1)\n",
    "        elif pivot_alg == 'random':\n",
    "            ind_pivot = td.Categorical(\n",
    "                probs=face.float()/(face.float().sum(-1, keepdims=True))\n",
    "            ).sample()\n",
    "        # Select a batch of pivots \n",
    "        # [B, K]\n",
    "        pivot_indicator = torch.nn.functional.one_hot(ind_pivot, K).bool()\n",
    "        # All non-zero coordinates but the pivot\n",
    "        # [B, K]\n",
    "        others = torch.logical_xor(face, pivot_indicator)\n",
    "        # The value of the pivot coordinate\n",
    "        # [B]\n",
    "        t = (y * pivot_indicator.float()).sum(-1)\n",
    "        # Pivot mean and variance\n",
    "        # [B]\n",
    "        t_mean = torch.where(pivot_indicator, loc, torch.zeros_like(loc)).sum(-1)\n",
    "        t_var = torch.where(pivot_indicator, var, torch.zeros_like(var)).sum(-1)\n",
    "\n",
    "        # Difference with respect to the pivot\n",
    "        # [B, K]\n",
    "        y_diff = torch.where(others, y - t.unsqueeze(-1), torch.zeros_like(y))\n",
    "        # [B, K]\n",
    "        mean_diff = torch.where(\n",
    "            others, \n",
    "            loc - t_mean.unsqueeze(-1),\n",
    "            torch.zeros_like(loc)\n",
    "        )\n",
    "        \n",
    "        # Joint log pdf for the non-zeros\n",
    "        # [B, K, K]    \n",
    "        diag = torch.diag_embed(torch.where(others, var, torch.ones_like(var)))\n",
    "        offset = t_var.unsqueeze(-1).unsqueeze(-1)\n",
    "        # We need a multivariate normal for the non-zero coordinates in `other`\n",
    "        # but to batch mvns we will need to use K-by-K covariances\n",
    "        # we can do so by embedding the lower-dimensional mvn in a higher dimensional mvn\n",
    "        # with cov=I.\n",
    "        # [B, K, K]\n",
    "        cov_mask = others.unsqueeze(-1) * others.unsqueeze(-2)\n",
    "        cov = torch.where(cov_mask, diag + offset, diag)\n",
    "        # This computes log prob of y[other] under  the lower dimensional mvn\n",
    "        # times log N(0|0,1) for the other dimensions\n",
    "        # [B]\n",
    "        log_prob = td.MultivariateNormal(mean_diff, cov).log_prob(y_diff)\n",
    "        # so we discount the contribution from the masked coordinates\n",
    "        # [B, K]\n",
    "        log_prob0 = td.Normal(torch.zeros_like(mean_diff), torch.ones_like(mean_diff)).log_prob(torch.zeros_like(y_diff)) \n",
    "        log_prob = log_prob - torch.where(others, torch.zeros_like(log_prob0), log_prob0).sum(-1)\n",
    "\n",
    "        # Joint log prob for the zeros (needs the cdf)\n",
    "        # [B]\n",
    "        constant_term = 1. / torch.where(face, 1./var, torch.zeros_like(var)).sum(-1)\n",
    "        # Again, we aim to reason with lower-dimensional mvns via \n",
    "        # the td.MultivariateNormal interface. For that, I will mask the coordinates in face.\n",
    "        # The non-zeros get a tiny variance\n",
    "        # [B, K, K]\n",
    "        diag_corrected = torch.diag_embed(torch.where(face, torch.zeros_like(var) + tiny, var)) \n",
    "        # [B, 1, 1]\n",
    "        offset_corrected = constant_term.unsqueeze(-1).unsqueeze(-1)\n",
    "        # These are the zeros only.\n",
    "        # [B, K, K]\n",
    "        cov_corrected_mask = torch.logical_not(face).unsqueeze(-1) * torch.logical_not(face.unsqueeze(-2))    \n",
    "        cov_corrected = torch.where(cov_corrected_mask, diag_corrected + offset_corrected, diag_corrected)    \n",
    "\n",
    "        # The non-zeros get a large negative mean.\n",
    "        # [B]\n",
    "        mean_constant_term = constant_term * torch.where(face, (y - loc)/var, torch.zeros_like(y)).sum(-1)\n",
    "        # [B, K]\n",
    "        #  see that for non-zeros I move the location to something extremely negative\n",
    "        #  in combination with tiny variace this makes the density of 0 evaluate to 0\n",
    "        #  and the cdf of 0 evaluate to 1, for those coordinates\n",
    "        mean_corrected = torch.where(face, torch.zeros_like(y) - huge, loc + mean_constant_term.unsqueeze(-1))\n",
    "\n",
    "        # [B]\n",
    "        cdf = multivariate_normal_cdf(\n",
    "            torch.zeros_like(y),\n",
    "            mean_corrected, cov_corrected\n",
    "        )\n",
    "        log_cdf = cdf.log()\n",
    "\n",
    "        # [B]\n",
    "        log_det = face.float().sum(-1).log()\n",
    "\n",
    "        # [B]\n",
    "        return log_prob + log_cdf + log_det\n",
    "\n",
    "\n",
    "@td.register_kl(GaussianSparsemax, GaussianSparsemax)\n",
    "def _kl_gaussiansparsemax_gaussiansparsemax(p, q):\n",
    "    x = p.rsample()\n",
    "    return p.log_prob(x) - q.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density(y, base_dists, ind_pivot=0):\n",
    "    \"\"\"\n",
    "    Evaluate the density of y. Can optionally pick a pivot.\n",
    "    y_nz contains only the non-zeros, corresponding to the non-masked\n",
    "    entries of the face.\n",
    "    \"\"\"\n",
    "    face = y > 0\n",
    "    y_nz = y[np.nonzero(face)[0]]\n",
    "    from scipy.stats import multivariate_normal\n",
    "\n",
    "    nz = np.nonzero(face)[0]\n",
    "    zeros = np.nonzero(1-np.array(face))[0]\n",
    "    pivot = nz[ind_pivot]\n",
    "    nz_minus_pivot = np.array(list(set(nz) - {pivot}))\n",
    "    y = np.zeros(len(face))\n",
    "    y[nz] = y_nz\n",
    "\n",
    "    # Compute density contribution by the nonzeros - requires pdf.\n",
    "    y_diff = np.array([y[s] - y[pivot] for s in nz_minus_pivot])\n",
    "    mean_diff = np.array([base_dists[s].mean() - base_dists[pivot].mean() for s in nz_minus_pivot])\n",
    "    if len(nz_minus_pivot):\n",
    "        cov = np.diag(np.array([base_dists[s].var() for s in nz_minus_pivot])) + base_dists[pivot].var()\n",
    "        val = multivariate_normal(mean=mean_diff, cov=cov).pdf(y_diff)\n",
    "    else:\n",
    "        val = 1\n",
    "    \n",
    "    # Compute density contribution by the zeros - requires cdf.\n",
    "    if len(zeros):\n",
    "        constant_term = 1 / sum([1/base_dists[s].var() for s in nz])\n",
    "        cov_corrected = np.diag(np.array([base_dists[r].var() for r in zeros])) + constant_term\n",
    "        mean_constant_term = (constant_term *\n",
    "                              sum(np.array([(y[s] - base_dists[s].mean())/base_dists[s].var() for s in nz])))\n",
    "        mean_corrected = np.array([base_dists[r].mean() + mean_constant_term for r in zeros])\n",
    "        cdf = multivariate_normal(mean=mean_corrected, cov=cov_corrected).cdf(np.zeros(len(zeros)))\n",
    "        val *= cdf    \n",
    "    val *= sum(face)  # This is the determinant of the transformation Y -> U = g(Y) for the nonzeros.\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=5 K=1\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=1\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=2\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=2\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=3\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=3\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=4\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=4\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=5\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=5\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=10\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n",
      "batch_size=5 K=10\n",
      " pass pivot='first': True\n",
      " pass pivot='random': True\n",
      " pass gradient check: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "for K in [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 10, 10]:\n",
    "    print(f\"batch_size={batch_size} K={K}\")\n",
    "    all_y = []\n",
    "    all_u = []\n",
    "    all_s = []\n",
    "    all_r1 = []\n",
    "    for n in range(batch_size):\n",
    "        u = torch.tensor(np.random.normal(0., 1., size=K), requires_grad=True)\n",
    "        s = torch.tensor(np.random.normal(1., 1., size=K), requires_grad=True)\n",
    "        s = torch.nn.functional.softplus(s)\n",
    "        all_u.append(u)\n",
    "        all_s.append(s)\n",
    "        y = GaussianSparsemax(u, s).sample()\n",
    "        all_y.append(y)\n",
    "        all_r1.append(np.log(density(y.detach().numpy(), [st.norm(loc=u.detach().numpy()[k], scale=s.detach().numpy()[k]) for k in range(K)])))    \n",
    "\n",
    "    # make a batch\n",
    "    Y = torch.stack(all_y)\n",
    "    U = torch.stack(all_u)\n",
    "    S = torch.stack(all_s)\n",
    "    all_r2 = GaussianSparsemax(U, S).log_prob(Y, pivot_alg='first')\n",
    "    all_r3 = GaussianSparsemax(U, S).log_prob(Y, pivot_alg='random')\n",
    "\n",
    "    print(\" pass pivot='first':\", torch.isclose(torch.tensor(all_r1), all_r2, atol=1e-3).all().item())\n",
    "    print(\" pass pivot='random':\", torch.isclose(torch.tensor(all_r1), all_r3, atol=1e-3).all().item())\n",
    "    \n",
    "    def get_loss(values, locs, scales, reduce=True):\n",
    "        loss = - GaussianSparsemax(locs, scales).log_prob(values)\n",
    "        return loss.mean() if reduce else loss\n",
    "    \n",
    "    grad_check = torch.autograd.gradcheck(\n",
    "        get_loss, \n",
    "        [Y, U, S], \n",
    "        eps=1e-02, \n",
    "        atol=0.1, \n",
    "        rtol=0.001, \n",
    "        raise_exception=True, \n",
    "        check_sparse_nnz=False, \n",
    "        nondet_tol=0.1, \n",
    "        check_undefined_grad=True, \n",
    "        check_grad_dtypes=False, \n",
    "        check_batched_grad=False\n",
    "    )\n",
    "    print(\" pass gradient check:\", grad_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = GaussianSparsemax(td.Normal(0., 1.).sample((10,)), td.Gamma(1., 1.).sample((10,)))\n",
    "q = GaussianSparsemax(td.Normal(0., 1.).sample((10,)), td.Gamma(1., 1.).sample((10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.5300e-06), tensor(-3.9628e-07, dtype=torch.float64))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p, p), td.kl_divergence(p.expand((100,)), p.expand((100,))).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.6511)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.3027, dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.kl_divergence(p.expand((1000,)), q.expand((1000,))).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = p.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9562, dtype=torch.float64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p.log_prob(x) - q.log_prob(x)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
